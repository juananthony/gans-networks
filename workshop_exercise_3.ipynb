{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de workshop_exercise_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juananthony/gans-networks/blob/master/workshop_exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XV_xUbZR_r",
        "colab_type": "text"
      },
      "source": [
        "Paso 1 - Instalando paquetes en Notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBzvq-EYrz9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODC2K6_9r2rK",
        "colab_type": "text"
      },
      "source": [
        "Paso 2 - Iniciando TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9gG5Hd8uKME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d59b3203-970e-4bf5-eda5-0334d7dc04ad"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFlJRt6dZWEK",
        "colab_type": "text"
      },
      "source": [
        "Paso 3 - Importando Paquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kobbP1saZaia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, Conv2D, Dropout, Flatten\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhN_cFZCaseG",
        "colab_type": "text"
      },
      "source": [
        "Paso 4 - Definici贸n de variable globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSkwpkDayLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "WIDTH = 28\n",
        "HEIGHT = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8oC0wBpZsoQ",
        "colab_type": "text"
      },
      "source": [
        "Paso 5 - Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ2ruFguZvkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data_set(height, weight, normalization_value):\n",
        "  (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "  train_images = train_images.reshape(train_images.shape[0], height, weight, 1).astype('float32')\n",
        "  train_images = (train_images - normalization_value) / normalization_value\n",
        "  return train_images, train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3OJN8ofcFXx",
        "colab_type": "text"
      },
      "source": [
        "Paso 6: Creaci贸n del modelo generador (Generator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6d2JgY_cRQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator_model(name, height, width, dims=1):\n",
        "    \n",
        "  model = Sequential()\n",
        "  model.add(Dense(7*7*256, use_bias=False, input_shape=(height*width, )))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "\n",
        "  model.add(Reshape((7, 7, 256)))\n",
        "\n",
        "  model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "\n",
        "  model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "\n",
        "  model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt1bqpiw80cg",
        "colab_type": "text"
      },
      "source": [
        "Paso 7: Creaci贸n del modelo discriminador (Discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJz8qEkk9P7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_model(name, height, width, dims):\n",
        "    \n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[height, width, dims]))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDGElH3W9wD5",
        "colab_type": "text"
      },
      "source": [
        "Paso 8 - Definici贸n de la clase Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN7ZkSX793sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "\n",
        "  def __init__(self, height, width, dims):\n",
        "  \n",
        "    self.__generator = build_generator_model('generator_test', height, width, dims)\n",
        "    self.__discriminator = build_discriminator_model('discriminator_test', height, width, dims)\n",
        "    self.__cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    self.__generator_optimizer = optimizers.Adam(1e-4)\n",
        "    self.__discriminator_optimizer = optimizers.Adam(1e-4)\n",
        "\n",
        "    self.__checkpoint_dir = './training_checkpoints'\n",
        "    self.__checkpoint_prefix = os.path.join(self.__checkpoint_dir, \"ckpt\")\n",
        "\n",
        "    self.__checkpoint = tf.train.Checkpoint(generator_optimizer=self.__generator_optimizer,\n",
        "                                 discriminator_optimizer=self.__discriminator_optimizer,\n",
        "                                 generator=self.__generator,\n",
        "                                 discriminator=self.__discriminator)\n",
        "    \n",
        "    self.__checkpoint.restore(tf.train.latest_checkpoint(self.__checkpoint_dir))\n",
        "\n",
        "\n",
        "  def generator_summary(self):\n",
        "    return self.__generator.summary()\n",
        "\n",
        "\n",
        "  def discriminator_summary(self):\n",
        "    return self.__discriminator.summary() \n",
        "\n",
        "\n",
        "  @property\n",
        "  def generator(self):\n",
        "    return self.__generator\n",
        "\n",
        "\n",
        "  @property\n",
        "  def discriminator(self):\n",
        "    return self.__discriminator\n",
        "\n",
        "\n",
        "  @property\n",
        "  def generator_optimizer(self):\n",
        "    return self.__generator_optimizer\n",
        "  \n",
        "\n",
        "  @property\n",
        "  def discriminator_optimizer(self):\n",
        "    return self.__discriminator_optimizer\n",
        "\n",
        "  \n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def discriminator_loss(self, real_output, fake_output):\n",
        "    real_loss = self.__cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = self.__cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "  \n",
        "\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def generator_loss(self, fake_output):\n",
        "    return self.__cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "  \n",
        "  def create_checkpoint(self):\n",
        "    self.__checkpoint.save(file_prefix = self.__checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UWM4JFk-CAm",
        "colab_type": "text"
      },
      "source": [
        "Paso 9 - Definici贸n de la funci贸n de generaci贸n de ejemplos y entrenamiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjwWVHdO_qvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, writer, images, example_num, example_shape, epoch):\n",
        "\n",
        "  # Definiremos un conjunto de ejemplos de tipo aleatorio\n",
        "  examples = tf.random.normal([example_num, example_shape])\n",
        "\n",
        "  with writer.as_default():\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "      generated_images = model.generator(examples, training=True)\n",
        "      real_output = model.discriminator(images, training=True)\n",
        "      fake_output = model.discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = model.generator_loss(fake_output)\n",
        "      disc_loss = model.discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    tf.summary.scalar(\"generator loss\", gen_loss, step=epoch+1)\n",
        "    tf.summary.scalar(\"discriminator loss\", disc_loss, step=epoch+1)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, model.generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, model.discriminator.trainable_variables)\n",
        "    model.generator_optimizer.apply_gradients(zip(gradients_of_generator, model.generator.trainable_variables))\n",
        "    model.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, model.discriminator.trainable_variables))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASsLYaYfsoMQ",
        "colab_type": "text"
      },
      "source": [
        "Paso 10 - Visualizaci贸n y test del proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uln93pqIvks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Para poder comprobar como evoluciona nuestra red vamos a crear una funci贸n de test que nos permitir谩 analizar la evoluci贸n de nuestra red en cada iteraci贸n. Para ello crearemos una funci贸n denominada generate_and_test_images de tipo tf.function (). Esta funci贸n recibir谩 5 par谩metros de entrada:\n",
        "\n",
        "model (Model) se corresponde con el modelo que est谩 siendo entrenado.\n",
        "writer (TF Writer) se corresponde con el objeto utilizado para almacena la informaci贸n para evaluaci贸n el proceso de entrenamiento.\n",
        "epoch (int) se corresponde con la iteraci贸n actual del proceso de entrenamiento.\n",
        "num_examples (int) se corresponde con el numero de imagenes que ser谩n generado en pasa iteraci贸n de entrenamiento.\n",
        "example_shape (tupple(int, int)) se corresponde con la estructura de la imagenes.\n",
        "\"\"\"\n",
        "def generate_and_test_images(model, writer, epoch, num_examples, example_size):\n",
        "\n",
        "  # Definiremos un conjunto de ejemplos de tipo aleatorio para testear nuestra RGA.\n",
        "  examples = tf.random.normal([num_examples, example_size])\n",
        "\n",
        "  # Ejecutaremos el proceso bajo el objeto writer con el objetivo de almacenar la informaci贸n obtenida sobre el conjunto de test aleatorio.\n",
        "  with writer.as_default():\n",
        "\n",
        "    # Prediciremos el valor del conjunto de imagenes aleatorio generadas previamente y calcularemo el valor de los para cada una de ellas.\n",
        "    predictions = model.generator(examples, training=False)\n",
        "    test_loss = model.generator_loss(predictions)\n",
        "\n",
        "    # Almacenaremos los valores de loss en el writer.\n",
        "    tf.summary.scalar(\"test loss\", test_loss, step=epoch+1)\n",
        "\n",
        "    # generaremos una imagen con todos los ejemplos generados\n",
        "    fig = plt.figure(figsize=(3,3))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 2048 + 2048, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuFumGOswvU",
        "colab_type": "text"
      },
      "source": [
        "Paso 11 - Definici贸n de bucle de entrenamiento (Funci贸n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSS-dyRyDyaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "d8300a28-5f0d-43fc-9916-ec287ff2d585"
      },
      "source": [
        "\"\"\"\n",
        "Una vez que se han definido todas las variables y funciones necesarias para el proceso de aprendizaje, podemos construir el bucle en tensorflow. Esta funci贸n estar谩 formada por 5 param茅tros:\n",
        "\n",
        "dataset (TensorFlow Slice) se corresponde con el conjunto de ejemplo de entrenamiento.\n",
        "epoch (int) se corresponde con el n煤mero de iteraciones del proceso de entrenamiento.\n",
        "example_num\n",
        "weight (int) se corresponde con el ancho de la imagenes que se utilizar谩n en el proceso.\n",
        "height (int) se corresponde con el ancho de la imagenes que se utilizar谩n en el proceso.\n",
        "log_dir (str) se corresponde con la ruta donde se almacenar谩n los ficheros de log\n",
        "\"\"\"\n",
        "def train(dataset, epochs, example_num, weight, height, log_dir):\n",
        "\n",
        "  model = Model(weight, height, 1)\n",
        "  model.generator.summary()\n",
        "  model.discriminator_summary()\n",
        "  \"\"\"\n",
        "  writer = tf.summary.create_file_writer(log_dir)\n",
        "  \n",
        "  example_size = weight*height\n",
        "\n",
        "  # Ejecutamos el bucle de entrenamiento en base al n煤mero de iteraciones.\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    # Para cada conjunto de im谩genes presentes en el dataset realizamos el proceso de entrenamiento.\n",
        "    for image_batch in dataset:\n",
        "      train_step(model, writer, image_batch, example_num, example_size, epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    # Para cada iteraci贸n aplicamos un proceso de test.\n",
        "    generate_and_test_images(model, writer, epoch + 1, 9, example_size)\n",
        "\n",
        "    # Cada 15 iteraciones almacenamos el estado del modelo con el objetivo de recuperarlo en caso de que existe \n",
        "    # alg煤n problema durante la ejecuci贸n o en caso de que iniciomes un nuevo proceso de entrenamiento.\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      model.create_checkpoint()\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  display.clear_output(wait=True)\n",
        "\n",
        "  return model\n",
        "  \"\"\"\n",
        "\n",
        "log_dir = \"./logs/eager\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train(None, 1, 10, WIDTH, HEIGHT, log_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 12544)             9834496   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_5 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 7, 7, 128)         819200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 14, 14, 64)        204800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         1600      \n",
            "=================================================================\n",
            "Total params: 10,911,040\n",
            "Trainable params: 10,885,568\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 212,865\n",
            "Trainable params: 212,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckdHnLuXswEW",
        "colab_type": "text"
      },
      "source": [
        "Paso 12 - Ejecuci贸n del proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPNNPPyuHI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xc1wZRLtAyA",
        "colab_type": "text"
      },
      "source": [
        "Paso 13 - Visualizaci贸n de los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C04jR-juGHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdVapxtBtCGe",
        "colab_type": "text"
      },
      "source": [
        "Paso 14: Puesta en funcionamiento de la RGA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZvPd0uttAM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5flHnl9HuIQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
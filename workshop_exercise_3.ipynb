{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de workshop_exercise_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juananthony/gans-networks/blob/master/workshop_exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XV_xUbZR_r",
        "colab_type": "text"
      },
      "source": [
        "Paso 1 - Instalando paquetes en Notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBzvq-EYrz9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODC2K6_9r2rK",
        "colab_type": "text"
      },
      "source": [
        "Paso 2 - Iniciando TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9gG5Hd8uKME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d59b3203-970e-4bf5-eda5-0334d7dc04ad"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFlJRt6dZWEK",
        "colab_type": "text"
      },
      "source": [
        "Paso 3 - Importando Paquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kobbP1saZaia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, Conv2D, Dropout, Flatten\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhN_cFZCaseG",
        "colab_type": "text"
      },
      "source": [
        "Paso 4 - Definición de variable globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSkwpkDayLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "WIDTH = 28\n",
        "HEIGHT = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8oC0wBpZsoQ",
        "colab_type": "text"
      },
      "source": [
        "Paso 5 - Carga de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ2ruFguZvkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data_set(height, weight, normalization_value):\n",
        "  (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "  train_images = train_images.reshape(train_images.shape[0], height, weight, 1).astype('float32')\n",
        "  train_images = (train_images - normalization_value) / normalization_value\n",
        "  return train_images, train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3OJN8ofcFXx",
        "colab_type": "text"
      },
      "source": [
        "Paso 6: Creación del modelo generador (Generator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6d2JgY_cRQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator_model(name, height, width, dims=1):\n",
        "    \n",
        "  model = Sequential()\n",
        "  model.add(Dense(7*7*256, use_bias=False, input_shape=(height*width, )))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "\n",
        "  model.add(Reshape((7, 7, 256)))\n",
        "\n",
        "  model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "\n",
        "  model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "\n",
        "  model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt1bqpiw80cg",
        "colab_type": "text"
      },
      "source": [
        "Paso 7: Creación del modelo discriminador (Discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJz8qEkk9P7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator_model(name, height, width, dims):\n",
        "    \n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[height, width, dims]))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDGElH3W9wD5",
        "colab_type": "text"
      },
      "source": [
        "Paso 8 - Definición de la clase Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN7ZkSX793sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "\n",
        "  def __init__(self, height, width, dims):\n",
        "  \n",
        "    self.__generator = build_generator_model('generator_test', height, width, dims)\n",
        "    self.__discriminator = build_discriminator_model('discriminator_test', height, width, dims)\n",
        "    self.__cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    self.__generator_optimizer = optimizers.Adam(1e-4)\n",
        "    self.__discriminator_optimizer = optimizers.Adam(1e-4)\n",
        "\n",
        "    self.__checkpoint_dir = './training_checkpoints'\n",
        "    self.__checkpoint_prefix = os.path.join(self.__checkpoint_dir, \"ckpt\")\n",
        "\n",
        "    self.__checkpoint = tf.train.Checkpoint(generator_optimizer=self.__generator_optimizer,\n",
        "                                 discriminator_optimizer=self.__discriminator_optimizer,\n",
        "                                 generator=self.__generator,\n",
        "                                 discriminator=self.__discriminator)\n",
        "    \n",
        "    self.__checkpoint.restore(tf.train.latest_checkpoint(self.__checkpoint_dir))\n",
        "\n",
        "\n",
        "  def generator_summary(self):\n",
        "    return self.__generator.summary()\n",
        "\n",
        "\n",
        "  def discriminator_summary(self):\n",
        "    return self.__discriminator.summary() \n",
        "\n",
        "\n",
        "  @property\n",
        "  def generator(self):\n",
        "    return self.__generator\n",
        "\n",
        "\n",
        "  @property\n",
        "  def discriminator(self):\n",
        "    return self.__discriminator\n",
        "\n",
        "\n",
        "  @property\n",
        "  def generator_optimizer(self):\n",
        "    return self.__generator_optimizer\n",
        "  \n",
        "\n",
        "  @property\n",
        "  def discriminator_optimizer(self):\n",
        "    return self.__discriminator_optimizer\n",
        "\n",
        "  \n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def discriminator_loss(self, real_output, fake_output):\n",
        "    real_loss = self.__cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = self.__cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "  \n",
        "\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def generator_loss(self, fake_output):\n",
        "    return self.__cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "  \n",
        "  def create_checkpoint(self):\n",
        "    self.__checkpoint.save(file_prefix = self.__checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UWM4JFk-CAm",
        "colab_type": "text"
      },
      "source": [
        "Paso 9 - Definición de la función de generación de ejemplos y entrenamiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjwWVHdO_qvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, writer, images, example_num, example_shape, epoch):\n",
        "\n",
        "  # Definiremos un conjunto de ejemplos de tipo aleatorio\n",
        "  examples = tf.random.normal([example_num, example_shape])\n",
        "\n",
        "  with writer.as_default():\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "      generated_images = model.generator(examples, training=True)\n",
        "      real_output = model.discriminator(images, training=True)\n",
        "      fake_output = model.discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = model.generator_loss(fake_output)\n",
        "      disc_loss = model.discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    tf.summary.scalar(\"generator loss\", gen_loss, step=epoch+1)\n",
        "    tf.summary.scalar(\"discriminator loss\", disc_loss, step=epoch+1)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, model.generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, model.discriminator.trainable_variables)\n",
        "    model.generator_optimizer.apply_gradients(zip(gradients_of_generator, model.generator.trainable_variables))\n",
        "    model.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, model.discriminator.trainable_variables))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASsLYaYfsoMQ",
        "colab_type": "text"
      },
      "source": [
        "Paso 10 - Visualización y test del proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uln93pqIvks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Para poder comprobar como evoluciona nuestra red vamos a crear una función de test que nos permitirá analizar la evolución de nuestra red en cada iteración. Para ello crearemos una función denominada generate_and_test_images de tipo tf.function (). Esta función recibirá 5 parámetros de entrada:\n",
        "\n",
        "model (Model) se corresponde con el modelo que está siendo entrenado.\n",
        "writer (TF Writer) se corresponde con el objeto utilizado para almacena la información para evaluación el proceso de entrenamiento.\n",
        "epoch (int) se corresponde con la iteración actual del proceso de entrenamiento.\n",
        "num_examples (int) se corresponde con el numero de imagenes que serán generado en pasa iteración de entrenamiento.\n",
        "example_shape (tupple(int, int)) se corresponde con la estructura de la imagenes.\n",
        "\"\"\"\n",
        "def generate_and_test_images(model, writer, epoch, num_examples, example_size):\n",
        "\n",
        "  # Definiremos un conjunto de ejemplos de tipo aleatorio para testear nuestra RGA.\n",
        "  examples = tf.random.normal([num_examples, example_size])\n",
        "\n",
        "  # Ejecutaremos el proceso bajo el objeto writer con el objetivo de almacenar la información obtenida sobre el conjunto de test aleatorio.\n",
        "  with writer.as_default():\n",
        "\n",
        "    # Prediciremos el valor del conjunto de imagenes aleatorio generadas previamente y calcularemo el valor de los para cada una de ellas.\n",
        "    predictions = model.generator(examples, training=False)\n",
        "    test_loss = model.generator_loss(predictions)\n",
        "\n",
        "    # Almacenaremos los valores de loss en el writer.\n",
        "    tf.summary.scalar(\"test loss\", test_loss, step=epoch+1)\n",
        "\n",
        "    # generaremos una imagen con todos los ejemplos generados\n",
        "    fig = plt.figure(figsize=(3,3))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 2048 + 2048, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuFumGOswvU",
        "colab_type": "text"
      },
      "source": [
        "Paso 11 - Definición de bucle de entrenamiento (Función)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSS-dyRyDyaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "d8300a28-5f0d-43fc-9916-ec287ff2d585"
      },
      "source": [
        "\"\"\"\n",
        "Una vez que se han definido todas las variables y funciones necesarias para el proceso de aprendizaje, podemos construir el bucle en tensorflow. Esta función estará formada por 5 paramétros:\n",
        "\n",
        "dataset (TensorFlow Slice) se corresponde con el conjunto de ejemplo de entrenamiento.\n",
        "epoch (int) se corresponde con el número de iteraciones del proceso de entrenamiento.\n",
        "example_num\n",
        "weight (int) se corresponde con el ancho de la imagenes que se utilizarán en el proceso.\n",
        "height (int) se corresponde con el ancho de la imagenes que se utilizarán en el proceso.\n",
        "log_dir (str) se corresponde con la ruta donde se almacenarán los ficheros de log\n",
        "\"\"\"\n",
        "def train(dataset, epochs, example_num, weight, height, log_dir):\n",
        "\n",
        "  model = Model(weight, height, 1)\n",
        "  model.generator.summary()\n",
        "  model.discriminator_summary()\n",
        "  \"\"\"\n",
        "  writer = tf.summary.create_file_writer(log_dir)\n",
        "  \n",
        "  example_size = weight*height\n",
        "\n",
        "  # Ejecutamos el bucle de entrenamiento en base al número de iteraciones.\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    # Para cada conjunto de imágenes presentes en el dataset realizamos el proceso de entrenamiento.\n",
        "    for image_batch in dataset:\n",
        "      train_step(model, writer, image_batch, example_num, example_size, epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    # Para cada iteración aplicamos un proceso de test.\n",
        "    generate_and_test_images(model, writer, epoch + 1, 9, example_size)\n",
        "\n",
        "    # Cada 15 iteraciones almacenamos el estado del modelo con el objetivo de recuperarlo en caso de que existe \n",
        "    # algún problema durante la ejecución o en caso de que iniciomes un nuevo proceso de entrenamiento.\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      model.create_checkpoint()\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  display.clear_output(wait=True)\n",
        "\n",
        "  return model\n",
        "  \"\"\"\n",
        "\n",
        "log_dir = \"./logs/eager\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train(None, 1, 10, WIDTH, HEIGHT, log_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 12544)             9834496   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_5 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 7, 7, 128)         819200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 14, 14, 64)        204800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         1600      \n",
            "=================================================================\n",
            "Total params: 10,911,040\n",
            "Trainable params: 10,885,568\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 212,865\n",
            "Trainable params: 212,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckdHnLuXswEW",
        "colab_type": "text"
      },
      "source": [
        "Paso 12 - Ejecución del proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPNNPPyuHI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xc1wZRLtAyA",
        "colab_type": "text"
      },
      "source": [
        "Paso 13 - Visualización de los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C04jR-juGHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdVapxtBtCGe",
        "colab_type": "text"
      },
      "source": [
        "Paso 14: Puesta en funcionamiento de la RGA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZvPd0uttAM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5flHnl9HuIQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}